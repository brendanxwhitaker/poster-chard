["1 Massachusetts Institute of Technology, 2 Ohio State University, 3 University of Chicago Luann Jung 1 , Brendan Whitaker 2 , Kyle Chard 3 (advisor), Aaron Elmore 3 (advisor) Measuring Swampiness: Quantifying Chaos in Large Heterogeneous Data Repositories Introduction We explore a novel clustering-based approach for organizing data “swamps” by automatically identifying latent content similar-ities between files. We developed a parallel pipeline that crawls large filesystems, collects key information regarding data composition and distribu-tion, and then clusters files automatically based on extracted content and metadata. To evaluate our methods we propose a novel method for quanti-fying the organization of a filesystem using a score based on the directory composition of the clusters. Evaluation on synthetic data Acknowledgements height = 2 N = 3 As a baseline for comparison, we generated 4 “perfect” datasets based on N -ary trees: - 2-ary 5-height (expect 32 clusters) - 6-ary 2-height (expect 36 clusters) - 3-ary 3-height (expect 27 clusters) - 40-ary 1-height (expect 40 clusters) N -ary trees - root node/directory has N children - each subsequent node has N children - extended to some height h - each leaf node is named after some word w - each leaf node contains 20 files - each file contains w repeated 100 times Baselines: - Naive tree distance between files in the filesystem (cohesion): - Silhouette score calculated using file system tree distance (cohesion and separation) Comparison of scores } = 5 Head Tail m = 68 Clusters Sets and elements Quantities Functions Frequency of a directory in a cluster Order directories by descending frequency Set of differences in directory frequency Index of largest drop in frequency Head and tail of the frequency distribution Logarithm-like function defined for base of 1 Frequency drop score for a cluster Frequency drop score Frequency drop score Clustering pipeline Metadata extraction Extract metadata regarding filetypes within the dataset File processing Unstructured text data: - filetypes for text (txt, pdf, doc, html) are converted to txt for easy processing - text data are tokenized, stemmed, counted, and vectorized - TFIDF matrix computed Structured tabular data: - filetypes for tabular data (csv, tsv, xls) are converted to csv for easy processing - schema extraction extracts headers for the data - pairwise Jaccard distance matrix computed Clustering Text data: k-means and the faster MiniBatch k-means Tabular data: agglomerative hierarchical clustering Optimizing k Cluster over a user-specified range of k values Select the k value that yields the highest frequency drop score Cleanliness score Report frequency drop score associated with best k value We'd like to thank Will Brackenbury and Tyler Skluzacek for their help in the brainstorming and editing processes. This work was supported by NSF REU 1757964 BigDataX: From theory to practice in Big Data computing at eXtreme scales. Acknowledgements 1 2 Files Directories 3 4 File A File B File C Tree distance - calculated as the number of directory changes needed until two files are in the same directory - measures how far apart files in the filesystem are Metadata extraction File processing CLEANLINESS MEASURE Text clustering Tabular clustering Optimize k DATA REPOSITORY pub8 (subset of CDIAC) References Paul Beckman, Tyler J Skluzacek, Kyle Chard, and Ian Foster. 2017. Skluma:A statistical learning pipeline for taming unkempt data repositories. In 29th International Conference on Scientific and Statistical Database Management. 41. Will Brackenbury, Rui Liu, Mainack Mondal, Aaron J. Elmore, Blase Ur, KyleChard, and Michael J. Franklin. 2018. Draining the Data Swamp: A Similarity-based Approach. In Proceedings of the Workshop on Human-In-the-Loop Data Analytics (HILDA’18) . ACM, New York, NY, USA, Article 13, 7 pages. https://-doi.org/10.1145/3209900.3209911 [1] [2] VIEW OUR CODE Evaluation on real data PUB8 CDIAC CDIAC "]